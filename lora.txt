key=lora_unet_down_blocks_0_attentions_0_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_down_blocks_0_attentions_0_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=320, out_features=320, bias=True)
key=lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=320, bias=False)
key=lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=320, out_features=320, bias=True)
key=lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=320, bias=False)
key=lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)
key=lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)
key=lora_unet_down_blocks_0_attentions_1_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_down_blocks_0_attentions_1_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=320, out_features=320, bias=True)
key=lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=320, bias=False)
key=lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=320, out_features=320, bias=True)
key=lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=320, bias=False)
key=lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)
key=lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)
key=lora_unet_down_blocks_1_attentions_0_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_down_blocks_1_attentions_0_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=640, out_features=640, bias=True)
key=lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=640, bias=False)
key=lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=640, out_features=640, bias=True)
key=lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=640, bias=False)
key=lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)
key=lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)
key=lora_unet_down_blocks_1_attentions_1_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_down_blocks_1_attentions_1_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=640, out_features=640, bias=True)
key=lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=640, bias=False)
key=lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=640, out_features=640, bias=True)
key=lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=640, bias=False)
key=lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)
key=lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)
key=lora_unet_down_blocks_2_attentions_0_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_down_blocks_2_attentions_0_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=1280, out_features=1280, bias=True)
key=lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=1280, bias=False)
key=lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=1280, out_features=1280, bias=True)
key=lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=1280, bias=False)
key=lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)
key=lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)
key=lora_unet_down_blocks_2_attentions_1_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_down_blocks_2_attentions_1_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=1280, out_features=1280, bias=True)
key=lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=1280, bias=False)
key=lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=1280, out_features=1280, bias=True)
key=lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=1280, bias=False)
key=lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)
key=lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)
key=lora_unet_mid_block_attentions_0_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_mid_block_attentions_0_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=1280, out_features=1280, bias=True)
key=lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=1280, bias=False)
key=lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=1280, out_features=1280, bias=True)
key=lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=1280, bias=False)
key=lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)
key=lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)
key=lora_unet_up_blocks_1_attentions_0_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_1_attentions_0_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=1280, out_features=1280, bias=True)
key=lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=1280, out_features=1280, bias=True)
key=lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)
key=lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)
key=lora_unet_up_blocks_1_attentions_1_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_1_attentions_1_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=1280, out_features=1280, bias=True)
key=lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=1280, out_features=1280, bias=True)
key=lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)
key=lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)
key=lora_unet_up_blocks_1_attentions_2_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_1_attentions_2_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=1280, out_features=1280, bias=True)
key=lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=1280, out_features=1280, bias=True)
key=lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=1280, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=1280, bias=False)
key=lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)
key=lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)
key=lora_unet_up_blocks_2_attentions_0_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_2_attentions_0_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=640, out_features=640, bias=True)
key=lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=640, out_features=640, bias=True)
key=lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)
key=lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)
key=lora_unet_up_blocks_2_attentions_1_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_2_attentions_1_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=640, out_features=640, bias=True)
key=lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=640, out_features=640, bias=True)
key=lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)
key=lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)
key=lora_unet_up_blocks_2_attentions_2_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_2_attentions_2_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=640, out_features=640, bias=True)
key=lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=640, out_features=640, bias=True)
key=lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=640, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=640, bias=False)
key=lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)
key=lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)
key=lora_unet_up_blocks_3_attentions_0_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_3_attentions_0_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=320, out_features=320, bias=True)
key=lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=320, out_features=320, bias=True)
key=lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)
key=lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)
key=lora_unet_up_blocks_3_attentions_1_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_3_attentions_1_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=320, out_features=320, bias=True)
key=lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=320, out_features=320, bias=True)
key=lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)
key=lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)     
key=lora_unet_up_blocks_3_attentions_2_proj_in.lora_down.weight temp_name=proj_in layer=LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_3_attentions_2_proj_out.lora_down.weight temp_name=proj_out layer=LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))
key=lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=320, out_features=320, bias=True)
key=lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight temp_name=to_k layer=Linear(in_features=768, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight temp_name=0 layer=Linear(in_features=320, out_features=320, bias=True)
key=lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight temp_name=to_q layer=Linear(in_features=320, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight temp_name=to_v layer=Linear(in_features=768, out_features=320, bias=False)
key=lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight temp_name=proj layer=LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)
key=lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight temp_name=2 layer=LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)  