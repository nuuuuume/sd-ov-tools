lora_te_text_model_encoder_layers_0_mlp_fc1.alpha torch.Size([])
lora_te_text_model_encoder_layers_0_mlp_fc1.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_0_mlp_fc1.lora_up.weight torch.Size([3072, 8])
lora_te_text_model_encoder_layers_0_mlp_fc2.alpha torch.Size([])
lora_te_text_model_encoder_layers_0_mlp_fc2.lora_down.weight torch.Size([8, 3072])
lora_te_text_model_encoder_layers_0_mlp_fc2.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_0_self_attn_k_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_0_self_attn_out_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_0_self_attn_out_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_0_self_attn_out_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_0_self_attn_q_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_0_self_attn_q_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_0_self_attn_q_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_0_self_attn_v_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_0_self_attn_v_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_0_self_attn_v_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_10_mlp_fc1.alpha torch.Size([])
lora_te_text_model_encoder_layers_10_mlp_fc1.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_10_mlp_fc1.lora_up.weight torch.Size([3072, 8])
lora_te_text_model_encoder_layers_10_mlp_fc2.alpha torch.Size([])
lora_te_text_model_encoder_layers_10_mlp_fc2.lora_down.weight torch.Size([8, 3072])
lora_te_text_model_encoder_layers_10_mlp_fc2.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_10_self_attn_k_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_10_self_attn_k_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_10_self_attn_k_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_10_self_attn_out_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_10_self_attn_out_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_10_self_attn_out_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_10_self_attn_q_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_10_self_attn_q_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_10_self_attn_q_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_10_self_attn_v_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_10_self_attn_v_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_10_self_attn_v_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_11_mlp_fc1.alpha torch.Size([])
lora_te_text_model_encoder_layers_11_mlp_fc1.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_11_mlp_fc1.lora_up.weight torch.Size([3072, 8])
lora_te_text_model_encoder_layers_11_mlp_fc2.alpha torch.Size([])
lora_te_text_model_encoder_layers_11_mlp_fc2.lora_down.weight torch.Size([8, 3072])
lora_te_text_model_encoder_layers_11_mlp_fc2.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_11_self_attn_k_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_11_self_attn_k_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_11_self_attn_k_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_11_self_attn_out_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_11_self_attn_out_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_11_self_attn_out_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_11_self_attn_q_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_11_self_attn_q_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_11_self_attn_q_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_11_self_attn_v_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_11_self_attn_v_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_11_self_attn_v_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_1_mlp_fc1.alpha torch.Size([])
lora_te_text_model_encoder_layers_1_mlp_fc1.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_1_mlp_fc1.lora_up.weight torch.Size([3072, 8])
lora_te_text_model_encoder_layers_1_mlp_fc2.alpha torch.Size([])
lora_te_text_model_encoder_layers_1_mlp_fc2.lora_down.weight torch.Size([8, 3072])
lora_te_text_model_encoder_layers_1_mlp_fc2.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_1_self_attn_k_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_1_self_attn_k_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_1_self_attn_k_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_1_self_attn_out_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_1_self_attn_out_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_1_self_attn_out_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_1_self_attn_q_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_1_self_attn_q_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_1_self_attn_q_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_1_self_attn_v_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_1_self_attn_v_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_1_self_attn_v_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_2_mlp_fc1.alpha torch.Size([])
lora_te_text_model_encoder_layers_2_mlp_fc1.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_2_mlp_fc1.lora_up.weight torch.Size([3072, 8])
lora_te_text_model_encoder_layers_2_mlp_fc2.alpha torch.Size([])
lora_te_text_model_encoder_layers_2_mlp_fc2.lora_down.weight torch.Size([8, 3072])
lora_te_text_model_encoder_layers_2_mlp_fc2.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_2_self_attn_k_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_2_self_attn_k_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_2_self_attn_k_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_2_self_attn_out_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_2_self_attn_out_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_2_self_attn_out_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_2_self_attn_q_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_2_self_attn_q_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_2_self_attn_q_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_2_self_attn_v_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_2_self_attn_v_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_2_self_attn_v_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_3_mlp_fc1.alpha torch.Size([])
lora_te_text_model_encoder_layers_3_mlp_fc1.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_3_mlp_fc1.lora_up.weight torch.Size([3072, 8])
lora_te_text_model_encoder_layers_3_mlp_fc2.alpha torch.Size([])
lora_te_text_model_encoder_layers_3_mlp_fc2.lora_down.weight torch.Size([8, 3072])
lora_te_text_model_encoder_layers_3_mlp_fc2.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_3_self_attn_k_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_3_self_attn_k_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_3_self_attn_k_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_3_self_attn_out_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_3_self_attn_out_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_3_self_attn_out_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_3_self_attn_q_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_3_self_attn_q_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_3_self_attn_q_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_3_self_attn_v_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_3_self_attn_v_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_3_self_attn_v_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_4_mlp_fc1.alpha torch.Size([])
lora_te_text_model_encoder_layers_4_mlp_fc1.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_4_mlp_fc1.lora_up.weight torch.Size([3072, 8])
lora_te_text_model_encoder_layers_4_mlp_fc2.alpha torch.Size([])
lora_te_text_model_encoder_layers_4_mlp_fc2.lora_down.weight torch.Size([8, 3072])
lora_te_text_model_encoder_layers_4_mlp_fc2.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_4_self_attn_k_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_4_self_attn_k_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_4_self_attn_k_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_4_self_attn_out_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_4_self_attn_out_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_4_self_attn_out_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_4_self_attn_q_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_4_self_attn_q_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_4_self_attn_q_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_4_self_attn_v_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_4_self_attn_v_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_4_self_attn_v_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_5_mlp_fc1.alpha torch.Size([])
lora_te_text_model_encoder_layers_5_mlp_fc1.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_5_mlp_fc1.lora_up.weight torch.Size([3072, 8])
lora_te_text_model_encoder_layers_5_mlp_fc2.alpha torch.Size([])
lora_te_text_model_encoder_layers_5_mlp_fc2.lora_down.weight torch.Size([8, 3072])
lora_te_text_model_encoder_layers_5_mlp_fc2.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_5_self_attn_k_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_5_self_attn_k_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_5_self_attn_k_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_5_self_attn_out_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_5_self_attn_out_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_5_self_attn_out_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_5_self_attn_q_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_5_self_attn_q_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_5_self_attn_q_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_5_self_attn_v_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_5_self_attn_v_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_5_self_attn_v_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_6_mlp_fc1.alpha torch.Size([])
lora_te_text_model_encoder_layers_6_mlp_fc1.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_6_mlp_fc1.lora_up.weight torch.Size([3072, 8])
lora_te_text_model_encoder_layers_6_mlp_fc2.alpha torch.Size([])
lora_te_text_model_encoder_layers_6_mlp_fc2.lora_down.weight torch.Size([8, 3072])
lora_te_text_model_encoder_layers_6_mlp_fc2.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_6_self_attn_k_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_6_self_attn_k_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_6_self_attn_k_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_6_self_attn_out_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_6_self_attn_out_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_6_self_attn_out_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_6_self_attn_q_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_6_self_attn_q_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_6_self_attn_q_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_6_self_attn_v_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_6_self_attn_v_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_6_self_attn_v_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_7_mlp_fc1.alpha torch.Size([])
lora_te_text_model_encoder_layers_7_mlp_fc1.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_7_mlp_fc1.lora_up.weight torch.Size([3072, 8])
lora_te_text_model_encoder_layers_7_mlp_fc2.alpha torch.Size([])
lora_te_text_model_encoder_layers_7_mlp_fc2.lora_down.weight torch.Size([8, 3072])
lora_te_text_model_encoder_layers_7_mlp_fc2.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_7_self_attn_k_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_7_self_attn_k_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_7_self_attn_k_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_7_self_attn_out_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_7_self_attn_out_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_7_self_attn_out_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_7_self_attn_q_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_7_self_attn_q_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_7_self_attn_q_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_7_self_attn_v_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_7_self_attn_v_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_7_self_attn_v_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_8_mlp_fc1.alpha torch.Size([])
lora_te_text_model_encoder_layers_8_mlp_fc1.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_8_mlp_fc1.lora_up.weight torch.Size([3072, 8])
lora_te_text_model_encoder_layers_8_mlp_fc2.alpha torch.Size([])
lora_te_text_model_encoder_layers_8_mlp_fc2.lora_down.weight torch.Size([8, 3072])
lora_te_text_model_encoder_layers_8_mlp_fc2.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_8_self_attn_k_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_8_self_attn_k_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_8_self_attn_k_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_8_self_attn_out_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_8_self_attn_out_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_8_self_attn_out_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_8_self_attn_q_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_8_self_attn_q_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_8_self_attn_q_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_8_self_attn_v_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_8_self_attn_v_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_8_self_attn_v_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_9_mlp_fc1.alpha torch.Size([])
lora_te_text_model_encoder_layers_9_mlp_fc1.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_9_mlp_fc1.lora_up.weight torch.Size([3072, 8])
lora_te_text_model_encoder_layers_9_mlp_fc2.alpha torch.Size([])
lora_te_text_model_encoder_layers_9_mlp_fc2.lora_down.weight torch.Size([8, 3072])
lora_te_text_model_encoder_layers_9_mlp_fc2.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_9_self_attn_k_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_9_self_attn_k_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_9_self_attn_k_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_9_self_attn_out_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_9_self_attn_out_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_9_self_attn_out_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_9_self_attn_q_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_9_self_attn_q_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_9_self_attn_q_proj.lora_up.weight torch.Size([768, 8])
lora_te_text_model_encoder_layers_9_self_attn_v_proj.alpha torch.Size([])
lora_te_text_model_encoder_layers_9_self_attn_v_proj.lora_down.weight torch.Size([8, 768])
lora_te_text_model_encoder_layers_9_self_attn_v_proj.lora_up.weight torch.Size([768, 8])
lora_unet_down_blocks_0_attentions_0_proj_in.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_0_proj_in.lora_down.weight torch.Size([8, 320, 1, 1])
lora_unet_down_blocks_0_attentions_0_proj_in.lora_up.weight torch.Size([320, 8, 1, 1])
lora_unet_down_blocks_0_attentions_0_proj_out.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_0_proj_out.lora_down.weight torch.Size([8, 320, 1, 1])
lora_unet_down_blocks_0_attentions_0_proj_out.lora_up.weight torch.Size([320, 8, 1, 1])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 320])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 320])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 320])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 320])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 320])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 320])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 320])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([2560, 8])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_1_proj_in.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_1_proj_in.lora_down.weight torch.Size([8, 320, 1, 1])
lora_unet_down_blocks_0_attentions_1_proj_in.lora_up.weight torch.Size([320, 8, 1, 1])
lora_unet_down_blocks_0_attentions_1_proj_out.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_1_proj_out.lora_down.weight torch.Size([8, 320, 1, 1])
lora_unet_down_blocks_0_attentions_1_proj_out.lora_up.weight torch.Size([320, 8, 1, 1])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 320])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 320])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 320])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 320])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 320])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 320])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 320])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([2560, 8])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([320, 8])
lora_unet_down_blocks_1_attentions_0_proj_in.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_0_proj_in.lora_down.weight torch.Size([8, 640, 1, 1])
lora_unet_down_blocks_1_attentions_0_proj_in.lora_up.weight torch.Size([640, 8, 1, 1])
lora_unet_down_blocks_1_attentions_0_proj_out.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_0_proj_out.lora_down.weight torch.Size([8, 640, 1, 1])
lora_unet_down_blocks_1_attentions_0_proj_out.lora_up.weight torch.Size([640, 8, 1, 1])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 640])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 640])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 640])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 640])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 640])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 640])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 640])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([5120, 8])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 2560])
lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_1_proj_in.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_1_proj_in.lora_down.weight torch.Size([8, 640, 1, 1])
lora_unet_down_blocks_1_attentions_1_proj_in.lora_up.weight torch.Size([640, 8, 1, 1])
lora_unet_down_blocks_1_attentions_1_proj_out.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_1_proj_out.lora_down.weight torch.Size([8, 640, 1, 1])
lora_unet_down_blocks_1_attentions_1_proj_out.lora_up.weight torch.Size([640, 8, 1, 1])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 640])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 640])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 640])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 640])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 640])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 640])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 640])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([5120, 8])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 2560])
lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([640, 8])
lora_unet_down_blocks_2_attentions_0_proj_in.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_0_proj_in.lora_down.weight torch.Size([8, 1280, 1, 1])
lora_unet_down_blocks_2_attentions_0_proj_in.lora_up.weight torch.Size([1280, 8, 1, 1])
lora_unet_down_blocks_2_attentions_0_proj_out.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_0_proj_out.lora_down.weight torch.Size([8, 1280, 1, 1])
lora_unet_down_blocks_2_attentions_0_proj_out.lora_up.weight torch.Size([1280, 8, 1, 1])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([10240, 8])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 5120])
lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_1_proj_in.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_1_proj_in.lora_down.weight torch.Size([8, 1280, 1, 1])
lora_unet_down_blocks_2_attentions_1_proj_in.lora_up.weight torch.Size([1280, 8, 1, 1])
lora_unet_down_blocks_2_attentions_1_proj_out.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_1_proj_out.lora_down.weight torch.Size([8, 1280, 1, 1])
lora_unet_down_blocks_2_attentions_1_proj_out.lora_up.weight torch.Size([1280, 8, 1, 1])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([1280, 8])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 1280])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([10240, 8])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 5120])
lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([1280, 8])
lora_unet_mid_block_attentions_0_proj_in.alpha torch.Size([])
lora_unet_mid_block_attentions_0_proj_in.lora_down.weight torch.Size([8, 1280, 1, 1])
lora_unet_mid_block_attentions_0_proj_in.lora_up.weight torch.Size([1280, 8, 1, 1])
lora_unet_mid_block_attentions_0_proj_out.alpha torch.Size([])
lora_unet_mid_block_attentions_0_proj_out.lora_down.weight torch.Size([8, 1280, 1, 1])
lora_unet_mid_block_attentions_0_proj_out.lora_up.weight torch.Size([1280, 8, 1, 1])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 1280])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([1280, 8])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 1280])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([1280, 8])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 1280])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([1280, 8])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 1280])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([1280, 8])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([1280, 8])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 1280])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([1280, 8])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 1280])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([1280, 8])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([1280, 8])
lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 1280])
lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([10240, 8])
lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 5120])
lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_0_proj_in.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_0_proj_in.lora_down.weight torch.Size([8, 1280, 1, 1])
lora_unet_up_blocks_1_attentions_0_proj_in.lora_up.weight torch.Size([1280, 8, 1, 1])
lora_unet_up_blocks_1_attentions_0_proj_out.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_0_proj_out.lora_down.weight torch.Size([8, 1280, 1, 1])
lora_unet_up_blocks_1_attentions_0_proj_out.lora_up.weight torch.Size([1280, 8, 1, 1])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([10240, 8])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 5120])
lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_1_proj_in.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_1_proj_in.lora_down.weight torch.Size([8, 1280, 1, 1])
lora_unet_up_blocks_1_attentions_1_proj_in.lora_up.weight torch.Size([1280, 8, 1, 1])
lora_unet_up_blocks_1_attentions_1_proj_out.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_1_proj_out.lora_down.weight torch.Size([8, 1280, 1, 1])
lora_unet_up_blocks_1_attentions_1_proj_out.lora_up.weight torch.Size([1280, 8, 1, 1])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([10240, 8])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 5120])
lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_2_proj_in.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_2_proj_in.lora_down.weight torch.Size([8, 1280, 1, 1])
lora_unet_up_blocks_1_attentions_2_proj_in.lora_up.weight torch.Size([1280, 8, 1, 1])
lora_unet_up_blocks_1_attentions_2_proj_out.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_2_proj_out.lora_down.weight torch.Size([8, 1280, 1, 1])
lora_unet_up_blocks_1_attentions_2_proj_out.lora_up.weight torch.Size([1280, 8, 1, 1])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([10240, 8])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 5120])
lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([1280, 8])
lora_unet_up_blocks_2_attentions_0_proj_in.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_0_proj_in.lora_down.weight torch.Size([8, 640, 1, 1])
lora_unet_up_blocks_2_attentions_0_proj_in.lora_up.weight torch.Size([640, 8, 1, 1])
lora_unet_up_blocks_2_attentions_0_proj_out.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_0_proj_out.lora_down.weight torch.Size([8, 640, 1, 1])
lora_unet_up_blocks_2_attentions_0_proj_out.lora_up.weight torch.Size([640, 8, 1, 1])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([5120, 8])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 2560])
lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_1_proj_in.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_1_proj_in.lora_down.weight torch.Size([8, 640, 1, 1])
lora_unet_up_blocks_2_attentions_1_proj_in.lora_up.weight torch.Size([640, 8, 1, 1])
lora_unet_up_blocks_2_attentions_1_proj_out.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_1_proj_out.lora_down.weight torch.Size([8, 640, 1, 1])
lora_unet_up_blocks_2_attentions_1_proj_out.lora_up.weight torch.Size([640, 8, 1, 1])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([5120, 8])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 2560])
lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_2_proj_in.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_2_proj_in.lora_down.weight torch.Size([8, 640, 1, 1])
lora_unet_up_blocks_2_attentions_2_proj_in.lora_up.weight torch.Size([640, 8, 1, 1])
lora_unet_up_blocks_2_attentions_2_proj_out.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_2_proj_out.lora_down.weight torch.Size([8, 640, 1, 1])
lora_unet_up_blocks_2_attentions_2_proj_out.lora_up.weight torch.Size([640, 8, 1, 1])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 640])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([5120, 8])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 2560])
lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([640, 8])
lora_unet_up_blocks_3_attentions_0_proj_in.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_0_proj_in.lora_down.weight torch.Size([8, 320, 1, 1])
lora_unet_up_blocks_3_attentions_0_proj_in.lora_up.weight torch.Size([320, 8, 1, 1])
lora_unet_up_blocks_3_attentions_0_proj_out.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_0_proj_out.lora_down.weight torch.Size([8, 320, 1, 1])
lora_unet_up_blocks_3_attentions_0_proj_out.lora_up.weight torch.Size([320, 8, 1, 1])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([2560, 8])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_1_proj_in.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_1_proj_in.lora_down.weight torch.Size([8, 320, 1, 1])
lora_unet_up_blocks_3_attentions_1_proj_in.lora_up.weight torch.Size([320, 8, 1, 1])
lora_unet_up_blocks_3_attentions_1_proj_out.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_1_proj_out.lora_down.weight torch.Size([8, 320, 1, 1])
lora_unet_up_blocks_3_attentions_1_proj_out.lora_up.weight torch.Size([320, 8, 1, 1])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([2560, 8])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_2_proj_in.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_2_proj_in.lora_down.weight torch.Size([8, 320, 1, 1])
lora_unet_up_blocks_3_attentions_2_proj_in.lora_up.weight torch.Size([320, 8, 1, 1])
lora_unet_up_blocks_3_attentions_2_proj_out.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_2_proj_out.lora_down.weight torch.Size([8, 320, 1, 1])
lora_unet_up_blocks_3_attentions_2_proj_out.lora_up.weight torch.Size([320, 8, 1, 1])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight torch.Size([8, 768])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight torch.Size([320, 8])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight torch.Size([8, 320])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight torch.Size([2560, 8])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.alpha torch.Size([])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight torch.Size([8, 1280])
lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight torch.Size([320, 8])
